{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Augmentation\n",
    "\n",
    "In this notebook, we will augment the Training Dataset with new examples of the \"Insincere\" label, to reduce label bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports\n",
    "\n",
    "I have found that Keras's implementation of NLP modules to be really useful, which are pulled from their repository here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd;\n",
    "import keras_text_preprocessing as text_preprocessor;\n",
    "import numpy as np;\n",
    "import random;\n",
    "from sklearn.neighbors import NearestNeighbors;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Loading. We are using the Cleaned and Preprocessed Data here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/cleaned_train.txt')\n",
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_text = data['cleaned_text']\n",
    "data_text = data_text.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To augment data, we use Word Embeddings. In this case, our embeddings size is 300."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Tokenization\n",
    "\n",
    "We will tokenize the sentences by using Keras's module. Words will be mapped to individual numbers in a sequence, so they can be used to index for corresponding word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 300\n",
    "MAX_WORDS = 200000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = text_preprocessor.Tokenizer(num_words=MAX_WORDS)\n",
    "tokenizer.fit_on_texts(data_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = tokenizer.texts_to_sequences(data_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ones = data[data['target'] == 1]\n",
    "ones_text = ones['cleaned_text'];\n",
    "ones_seq = tokenizer.texts_to_sequences(ones_text);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this augmentation task, we are using FastText Word Embeddings trained on Wikipedia Data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dict = {};\n",
    "with open('../Embeddings/crawl-%dd-2M.vec'%(embed_size), 'rb') as f:\n",
    "    for line in f:\n",
    "        splits = line.split();\n",
    "        word = splits[0];\n",
    "        vec = np.asarray(splits[1:], dtype='float32')\n",
    "        \n",
    "        embeddings_dict[word.decode()] = vec;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now use the Word Embeddings to build an Index mapping the words in our vocabulary to Vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_word = {};\n",
    "for word, item in word_index.items():\n",
    "    index_word[item] = word;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = min(MAX_WORDS, len(word_index))+1;\n",
    "embeddings_matrix = np.zeros((vocab_size, embed_size));\n",
    "\n",
    "for word, posit in word_index.items():\n",
    "    if posit >= vocab_size:\n",
    "        break;\n",
    "        \n",
    "    vec = embeddings_dict.get(word);\n",
    "    if vec is None:\n",
    "        vec = np.random.sample(embed_size);\n",
    "        embeddings_dict[word] = vec;\n",
    "    \n",
    "    embeddings_matrix[posit] = vec;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use K Nearest Neighbors algorithm to get the Nearest Neighbors of each word in our vocabulary, by using Vector Comparisons between the Word Embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_syns = 5;\n",
    "top_k = 20000;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "nearest_syns = NearestNeighbors(n_neighbors=total_syns+1).fit(embeddings_matrix) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbours_mat = nearest_syns.kneighbors(embeddings_matrix[1:top_k])[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "synonyms = {x[0]: x[1:] for x in neighbours_mat}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's view a few synonyms of words in our vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "infps : ['lithromantics', 'staffhunting', 'slughorn', 'occs']\n",
      "corruption : ['corrupt', 'corruptions', 'corrupted', 'malfeasance']\n",
      "fifty : ['twenty', 'thirty', 'forty', 'sixty']\n",
      "patton : ['davis', 'sherman', 'george', 'james']\n",
      "gm : ['gms', 'g', 'ml', 'sr']\n",
      "annoying : ['irritating', 'obnoxious', 'irksome', 'annoyingly']\n",
      "coupon : ['coupons', 'discount', 'discounts', 'promo']\n",
      "badge : ['badges', 'emblem', 'insignia', 'emblems']\n",
      "curiosity : ['curiousity', 'curosity', 'curious', 'fascination']\n",
      "could : ['would', 'might', 'can', 'should']\n"
     ]
    }
   ],
   "source": [
    "for posit in np.random.choice(top_k, 10):\n",
    "    print(index_word[posit] + \" : \" + str([index_word[synonyms[posit][i]] for i in range(total_syns-1)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmentation\n",
    "\n",
    "To augment the actual sentences, we will use the following strategy:\n",
    "\n",
    "Iterating over each sentence with a Target of 1, each word in a sentence will be replaced with a random synonym with a probability of 50%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_sentence(encoded_sentence, prob = 0.5):\n",
    "    for posit in range(len(encoded_sentence)):\n",
    "        if random.random() > prob:\n",
    "            try:\n",
    "                syns = synonyms[encoded_sentence[posit]];\n",
    "                rand_syn = np.random.choice(syns);\n",
    "                encoded_sentence[posit] = rand_syn;\n",
    "            except KeyError:\n",
    "                pass;\n",
    "    return encoded_sentence;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example sentence before replacement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'so like it marriage the american woman for with green it that more could they fee'"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join([index_word[idx] for idx in ones_seq[6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sequences = [];\n",
    "for ite in range(len(ones_seq)):\n",
    "    new_sequences.append(augment_sentence(ones_seq[ite]));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sentence after replacement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'it like it marriage the canadian lady the in greeen it but more could themselves pay'"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join([index_word[idx] for idx in new_sequences[6]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will add these new sentences to the original DataFrame and save to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.DataFrame(new_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['target'] = 1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.columns = ['cleaned_text', 'target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1303765"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_text_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.concat([data_text_label, new_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cleaned_text         0\n",
       "target          161450\n",
       "dtype: int64"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(combined_df == 1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.to_csv('data/augmented_quora_text.txt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
